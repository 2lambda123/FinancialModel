{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# trying to get better model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def get_featured_frame(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    DAY_VALUES = 1\n",
    "    MONTH_VALUES = 30\n",
    "    WEEK_VALUES = DAY_VALUES*7\n",
    "    YEAR_VALUES = DAY_VALUES*365\n",
    "    df['Time'] = pd.to_datetime(df.Time)\n",
    "    df = df.set_index('Time')\n",
    "    df = df.resample('D', convention='start').mean()\n",
    "    df['date'] = df.index.values\n",
    "    df['Avg'] =(df['Low'] + df['High'])/2\n",
    "    # time features\n",
    "    df['year'] = df['date'].apply(lambda x: x.year)\n",
    "    df['month'] = df['date'].apply(lambda x: x.month)\n",
    "    df['day'] = df['date'].apply(lambda x: x.day)\n",
    "    df['hour'] = df['date'].apply(lambda x: x.hour)\n",
    "    df['minute'] = df['date'].apply(lambda x: x.minute)\n",
    "    # Lagged Values\n",
    "    for unit, amount, shift_values in zip(['day', 'day', 'day', 'day', 'week', 'week', 'week', 'month', 'month', 'month', 'year'],[1,2,3,4,1,2,3,1,2,1],[DAY_VALUES, DAY_VALUES, DAY_VALUES, DAY_VALUES, WEEK_VALUES, WEEK_VALUES, WEEK_VALUES, MONTH_VALUES, MONTH_VALUES, YEAR_VALUES]):\n",
    "        for col in ['Open', 'Close', 'High', 'Low', 'Volume', 'Avg']:\n",
    "            new_col = \"{}_{}{}_before\".format(col, amount, unit)\n",
    "            df[new_col] = df[col].shift(amount*shift_values)\n",
    "    # Summary of values\n",
    "    for unit, amount, win_size in zip(['day', 'day', 'week', 'week','month', 'month', 'month'],[1,1,1,1,1,1,1],[2,5,2,3,1,2,3]):\n",
    "        for col in ['Open', 'Close', 'High', 'Low', 'Volume']:\n",
    "            roll_col = \"{}_av_{}{}_before_{}roll\".format(col, amount, unit, win_size)\n",
    "            shifted = \"{}_{}{}_before\".format(col, amount, unit)\n",
    "            df[roll_col] = (df[shifted].rolling(window=win_size)).mean()\n",
    "    # some stat of the values\n",
    "    for col in ['Open', 'Close', 'High', 'Low']:\n",
    "        window = df[col].expanding()\n",
    "        df[\"{}_max\".format(col)] = window.max()\n",
    "        df[\"{}_min\".format(col)] = window.min()\n",
    "        df[\"{}_avg\".format(col)] = window.mean()\n",
    "    df = df.drop(\"date\", axis=1)\n",
    "    # create the prediction column\n",
    "    df['next_rate'] = np.where(df['Avg'].shift(-1) > df['Avg'],1,-1)\n",
    "    df = df.dropna()\n",
    "    # sclae the values\n",
    "    scaler = MinMaxScaler()\n",
    "    df[df.columns] = scaler.fit_transform(df[df.columns])\n",
    "    print(df.info())\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 2190 entries, 2011-01-03 to 2016-12-31\n",
      "Freq: D\n",
      "Columns: 113 entries, Open to next_rate\n",
      "dtypes: float64(113)\n",
      "memory usage: 1.9 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "df = get_featured_frame(\"../res/EURUSD_15m_BID_01.01.2010-31.12.2016.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First we need to have some sort of auto_ML to find the best model\n",
    "## Let's build it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def auto_ml(X, Y):\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=.2, random_state=0)\n",
    "    clfs = {\n",
    "        \"LogisticRegression\": GridSearchCV(LogisticRegression(), {'C':[.001, .01, .1, 1, 10, 100],\n",
    "                                                               'penalty':['l2', 'l1']}, cv=5, n_jobs=-1),\n",
    "        \"SGDClassifier\": GridSearchCV(SGDClassifier(), {'max_iter':[1e1,1e2, 1e3],\n",
    "                                                     'loss':['log', 'modified_huber',\n",
    "                                                             'squared_hinge', 'perceptron'],\n",
    "                                                      'penalty':['elasticnet', 'l2', 'l1']}, cv=5, n_jobs=-1),\n",
    "        \"SVC\":GridSearchCV(SVC(), {'kernel': ['rbf'], 'gamma': [1e-3, 1e-4],\n",
    "                     'C': [1, 10, 100, 1000]}, cv=5, n_jobs=-1),\n",
    "        \"LinearSVC\": GridSearchCV(LinearSVC(), {'C':[1, 10, 100, 1000]}, cv=5, n_jobs=-1),\n",
    "        \"GaussianNB\": GridSearchCV(GaussianNB(),{'priors':[None]}, cv=5, n_jobs=-1),\n",
    "        \"KNeighborsClassifier\": GridSearchCV(KNeighborsClassifier(), {'n_neighbors':[1, 3, 5, 10, 100, 200],\n",
    "                     'weights':['uniform', 'distance'],\n",
    "                      'algorithm':['ball_tree', 'kd_tree', 'brute'], \n",
    "                      'p':[1, 2]}, cv=5, n_jobs=-1),\n",
    "        \"RandomForestClassifier\": GridSearchCV(RandomForestClassifier(), \n",
    "                       {\"n_estimators\":[10, 50, 100, 200],\n",
    "                       \"criterion\":[\"gini\", \"entropy\"]}, cv=5, n_jobs=-1),\n",
    "        \"AdaBoostClassifier\": GridSearchCV(AdaBoostClassifier(), \n",
    "                       {\"n_estimators\":[50, 100, 200]}, cv=5, n_jobs=-1),\n",
    "        \"GradientBoostingClassifier\": GridSearchCV(GradientBoostingClassifier(), \n",
    "                       {\"loss\":['deviance', 'exponential']}, cv=5, n_jobs=-1),\n",
    "        \"DecisionTreeClassifier\": GridSearchCV(DecisionTreeClassifier(), \n",
    "                       {\"criterion\":[\"gini\", \"entropy\"]}, cv=5, n_jobs=-1),\n",
    "    }\n",
    "    print(\"| model | train score | test score|\")\n",
    "    print(\"| --- | --- | --- |\")\n",
    "    best_score, best_test, best_clf = 0, 0, None\n",
    "    for clf in clfs.keys():\n",
    "        clfs[clf].fit(x_train, y_train)\n",
    "        score = clfs[clf].best_score_\n",
    "        test_score = clfs[clf].score(x_test, y_test)\n",
    "        print(\"| {} | {:.2f}% | {:.2f}% |\".format(clf, score, test_score))\n",
    "        if score > best_score and test_score > best_test:\n",
    "            best_score = score\n",
    "            best_test = test_score\n",
    "            best_clf = clfs[clf].best_estimator_\n",
    "    print(\"\\n\\n# the best classifier is `{}` with train score of {} and test score of {}\".format(best_clf, \n",
    "                                                                                          best_score, best_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's try the data with the given classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| model | train score | test score|\n",
      "| --- | --- | --- |\n",
      "| LogisticRegression | 0.60% | 0.56% |\n",
      "| SGDClassifier | 0.59% | 0.56% |\n",
      "| SVC | 0.58% | 0.57% |\n",
      "| LinearSVC | 0.60% | 0.55% |\n",
      "| GaussianNB | 0.48% | 0.49% |\n",
      "| KNeighborsClassifier | 0.57% | 0.53% |\n",
      "| RandomForestClassifier | 0.58% | 0.58% |\n",
      "| AdaBoostClassifier | 0.52% | 0.52% |\n",
      "| GradientBoostingClassifier | 0.53% | 0.53% |\n",
      "| DecisionTreeClassifier | 0.55% | 0.52% |\n",
      "\n",
      "\n",
      "# the best classifier is `DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')` with train score of 0 and test score of 0\n",
      "CPU times: user 23.6 s, sys: 944 ms, total: 24.6 s\n",
      "Wall time: 3min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "auto_ml(df.drop('next_rate', axis=1).values, df['next_rate'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| model | train score | test score|\n",
    "| --- | --- | --- |\n",
    "| LogisticRegression | 0.60% | 0.56% |\n",
    "| SGDClassifier | 0.59% | 0.58% |\n",
    "| SVC | 0.58% | 0.57% |\n",
    "| LinearSVC | 0.60% | 0.56% |\n",
    "| GaussianNB | 0.48% | 0.49% |\n",
    "| KNeighborsClassifier | 0.57% | 0.53% |\n",
    "| RandomForestClassifier | 0.58% | 0.58% |\n",
    "| AdaBoostClassifier | 0.52% | 0.52% |\n",
    "| GradientBoostingClassifier | 0.52% | 0.53% |\n",
    "| DecisionTreeClassifier | 0.54% | 0.56% |\n",
    "\n",
    "\n",
    "# the best classifier is `DecisionTreeClassifier` with train score of 0 and test score of 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's see the correlation between the features\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we are trying to see which varaibles have high correlation with the `Avg` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = df.corr()\n",
    "corrs = corr['Avg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "116"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corrs[abs(corrs)>0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Open', 'High', 'Low', 'Close', 'Avg', 'year', 'Open_1day_before',\n",
       "       'Close_1day_before', 'High_1day_before', 'Low_1day_before',\n",
       "       ...\n",
       "       'Close_avg^2', 'Close_avg^3', 'High_min^2', 'High_min^3', 'High_avg^2',\n",
       "       'High_avg^3', 'Low_min^2', 'Low_min^3', 'Low_avg^2', 'Low_avg^3'],\n",
       "      dtype='object', length=210)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(corrs[abs(corrs)>0.5].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SO we can see that only 87 features have more than abs(0.5) correlation, but wait, maybe the other values needs to be fitted from another degree !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating polynomials for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_poly_feats(df, degree=3, y_col='next_rate'):\n",
    "    from sklearn.preprocessing import PolynomialFeatures\n",
    "    df.dropna(inplace=True)\n",
    "    for feature in df.columns:\n",
    "        if feature == 'next_rate':\n",
    "            continue\n",
    "        poly_gen = PolynomialFeatures(degree=degree, include_bias=False)\n",
    "        polys = poly_gen.fit_transform(df[[feature]])\n",
    "        for column in range(degree):\n",
    "            if column > 0:\n",
    "                new_col_name = \"{}^{}\".format(feature, column+1)\n",
    "                df[new_col_name] = polys[:, column]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Avg</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>...</th>\n",
       "      <th>High_min^2</th>\n",
       "      <th>High_min^3</th>\n",
       "      <th>High_avg^2</th>\n",
       "      <th>High_avg^3</th>\n",
       "      <th>Low_max^2</th>\n",
       "      <th>Low_max^3</th>\n",
       "      <th>Low_min^2</th>\n",
       "      <th>Low_min^3</th>\n",
       "      <th>Low_avg^2</th>\n",
       "      <th>Low_avg^3</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2011-01-03</th>\n",
       "      <td>0.659563</td>\n",
       "      <td>0.659490</td>\n",
       "      <td>0.659557</td>\n",
       "      <td>0.659548</td>\n",
       "      <td>0.113870</td>\n",
       "      <td>0.659523</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.393609</td>\n",
       "      <td>0.246943</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.391333</td>\n",
       "      <td>0.244805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-01-04</th>\n",
       "      <td>0.663954</td>\n",
       "      <td>0.664138</td>\n",
       "      <td>0.663730</td>\n",
       "      <td>0.663851</td>\n",
       "      <td>0.147426</td>\n",
       "      <td>0.663934</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.393887</td>\n",
       "      <td>0.247205</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.391600</td>\n",
       "      <td>0.245056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-01-05</th>\n",
       "      <td>0.635120</td>\n",
       "      <td>0.635156</td>\n",
       "      <td>0.634780</td>\n",
       "      <td>0.634727</td>\n",
       "      <td>0.138942</td>\n",
       "      <td>0.634968</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.393697</td>\n",
       "      <td>0.247026</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.391401</td>\n",
       "      <td>0.244868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-01-06</th>\n",
       "      <td>0.605992</td>\n",
       "      <td>0.605905</td>\n",
       "      <td>0.605549</td>\n",
       "      <td>0.605563</td>\n",
       "      <td>0.126467</td>\n",
       "      <td>0.605727</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.393039</td>\n",
       "      <td>0.246407</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.390733</td>\n",
       "      <td>0.244242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-01-07</th>\n",
       "      <td>0.578792</td>\n",
       "      <td>0.579032</td>\n",
       "      <td>0.578433</td>\n",
       "      <td>0.578604</td>\n",
       "      <td>0.122968</td>\n",
       "      <td>0.578733</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.391955</td>\n",
       "      <td>0.245388</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.389636</td>\n",
       "      <td>0.243214</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 337 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Open      High       Low     Close    Volume       Avg  year  \\\n",
       "Time                                                                           \n",
       "2011-01-03  0.659563  0.659490  0.659557  0.659548  0.113870  0.659523   0.0   \n",
       "2011-01-04  0.663954  0.664138  0.663730  0.663851  0.147426  0.663934   0.0   \n",
       "2011-01-05  0.635120  0.635156  0.634780  0.634727  0.138942  0.634968   0.0   \n",
       "2011-01-06  0.605992  0.605905  0.605549  0.605563  0.126467  0.605727   0.0   \n",
       "2011-01-07  0.578792  0.579032  0.578433  0.578604  0.122968  0.578733   0.0   \n",
       "\n",
       "            month       day  hour    ...      High_min^2  High_min^3  \\\n",
       "Time                                 ...                               \n",
       "2011-01-03    0.0  0.066667   0.0    ...             1.0         1.0   \n",
       "2011-01-04    0.0  0.100000   0.0    ...             1.0         1.0   \n",
       "2011-01-05    0.0  0.133333   0.0    ...             1.0         1.0   \n",
       "2011-01-06    0.0  0.166667   0.0    ...             1.0         1.0   \n",
       "2011-01-07    0.0  0.200000   0.0    ...             1.0         1.0   \n",
       "\n",
       "            High_avg^2  High_avg^3  Low_max^2  Low_max^3  Low_min^2  \\\n",
       "Time                                                                  \n",
       "2011-01-03    0.393609    0.246943        0.0        0.0        1.0   \n",
       "2011-01-04    0.393887    0.247205        0.0        0.0        1.0   \n",
       "2011-01-05    0.393697    0.247026        0.0        0.0        1.0   \n",
       "2011-01-06    0.393039    0.246407        0.0        0.0        1.0   \n",
       "2011-01-07    0.391955    0.245388        0.0        0.0        1.0   \n",
       "\n",
       "            Low_min^3  Low_avg^2  Low_avg^3  \n",
       "Time                                         \n",
       "2011-01-03        1.0   0.391333   0.244805  \n",
       "2011-01-04        1.0   0.391600   0.245056  \n",
       "2011-01-05        1.0   0.391401   0.244868  \n",
       "2011-01-06        1.0   0.390733   0.244242  \n",
       "2011-01-07        1.0   0.389636   0.243214  \n",
       "\n",
       "[5 rows x 337 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_poly_feats(df.copy(), 3).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's see the correlation again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_poly_3 = generate_poly_feats(df.copy(), 3)\n",
    "corr = df_poly_3.corr()\n",
    "corrs = corr['Avg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "337"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "210"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corrs[abs(corrs)>0.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SO the poly features might be a good way for the model to get an idea about the data !\n",
    "## let's see the affect of the polys on the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the poly features\n",
    "df_poly_2 = generate_poly_feats(df.copy(), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# testing on second poly degree features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| model | train score | test score|\n",
      "| --- | --- | --- |\n",
      "| LogisticRegression | 0.60% | 0.60% |\n",
      "| SGDClassifier | 0.59% | 0.59% |\n",
      "| SVC | 0.57% | 0.55% |\n",
      "| LinearSVC | 0.59% | 0.61% |\n",
      "| GaussianNB | 0.49% | 0.49% |\n",
      "| KNeighborsClassifier | 0.58% | 0.59% |\n",
      "| RandomForestClassifier | 0.57% | 0.59% |\n",
      "| AdaBoostClassifier | 0.52% | 0.52% |\n",
      "| GradientBoostingClassifier | 0.53% | 0.53% |\n",
      "| DecisionTreeClassifier | 0.54% | 0.55% |\n",
      "\n",
      "\n",
      "# the best classifier is `DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')` with train score of 0 and test score of 0\n"
     ]
    }
   ],
   "source": [
    "auto_ml(df_poly_2.drop('next_rate', 1), df_poly_2['next_rate'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| model | train score | test score|\n",
    "| --- | --- | --- |\n",
    "| LogisticRegression | 0.60% | 0.60% |\n",
    "| SGDClassifier | 0.59% | 0.59% |\n",
    "| SVC | 0.57% | 0.55% |\n",
    "| LinearSVC | 0.59% | 0.61% |\n",
    "| GaussianNB | 0.49% | 0.49% |\n",
    "| KNeighborsClassifier | 0.58% | 0.59% |\n",
    "| RandomForestClassifier | 0.57% | 0.59% |\n",
    "| AdaBoostClassifier | 0.52% | 0.52% |\n",
    "| GradientBoostingClassifier | 0.53% | 0.53% |\n",
    "| DecisionTreeClassifier | 0.54% | 0.55% |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# testing on third poly degree features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| model | train score | test score|\n",
      "| --- | --- | --- |\n",
      "| LogisticRegression | 0.59% | 0.58% |\n",
      "| SGDClassifier | 0.58% | 0.58% |\n",
      "| SVC | 0.57% | 0.54% |\n",
      "| LinearSVC | 0.58% | 0.58% |\n",
      "| GaussianNB | 0.49% | 0.50% |\n",
      "| KNeighborsClassifier | 0.58% | 0.58% |\n",
      "| RandomForestClassifier | 0.57% | 0.59% |\n",
      "| AdaBoostClassifier | 0.52% | 0.52% |\n",
      "| GradientBoostingClassifier | 0.53% | 0.54% |\n",
      "| DecisionTreeClassifier | 0.54% | 0.54% |\n",
      "\n",
      "\n",
      "# the best classifier is `DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')` with train score of 0 and test score of 0\n"
     ]
    }
   ],
   "source": [
    "auto_ml(df_poly_3.drop('next_rate', 1), df_poly_3['next_rate'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| model | train score | test score|\n",
    "| --- | --- | --- |\n",
    "| LogisticRegression | 0.59% | 0.58% |\n",
    "| SGDClassifier | 0.58% | 0.58% |\n",
    "| SVC | 0.57% | 0.54% |\n",
    "| LinearSVC | 0.58% | 0.58% |\n",
    "| GaussianNB | 0.49% | 0.50% |\n",
    "| KNeighborsClassifier | 0.58% | 0.58% |\n",
    "| RandomForestClassifier | 0.57% | 0.59% |\n",
    "| AdaBoostClassifier | 0.52% | 0.52% |\n",
    "| GradientBoostingClassifier | 0.53% | 0.54% |\n",
    "| DecisionTreeClassifier | 0.54% | 0.54% |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's try with only the values that has high correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# training using `210` features with correlation >= 0.5\n",
      "| model | train score | test score|\n",
      "| --- | --- | --- |\n",
      "| LogisticRegression | 0.60% | 0.57% |\n",
      "| SGDClassifier | 0.59% | 0.59% |\n",
      "| SVC | 0.58% | 0.57% |\n",
      "| LinearSVC | 0.58% | 0.61% |\n",
      "| GaussianNB | 0.50% | 0.49% |\n",
      "| KNeighborsClassifier | 0.60% | 0.57% |\n",
      "| RandomForestClassifier | 0.58% | 0.56% |\n",
      "| AdaBoostClassifier | 0.53% | 0.48% |\n",
      "| GradientBoostingClassifier | 0.56% | 0.52% |\n",
      "| DecisionTreeClassifier | 0.57% | 0.52% |\n",
      "\n",
      "\n",
      "# the best classifier is `LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)` with train score of 0.6004566210045662 and test score of 0.5730593607305936\n",
      "\n",
      "---\n",
      "\n",
      "# training using `210` features with correlation >= 0.7\n",
      "| model | train score | test score|\n",
      "| --- | --- | --- |\n",
      "| LogisticRegression | 0.60% | 0.57% |\n",
      "| SGDClassifier | 0.59% | 0.58% |\n",
      "| SVC | 0.58% | 0.57% |\n",
      "| LinearSVC | 0.59% | 0.59% |\n",
      "| GaussianNB | 0.50% | 0.49% |\n",
      "| KNeighborsClassifier | 0.60% | 0.57% |\n",
      "| RandomForestClassifier | 0.58% | 0.54% |\n",
      "| AdaBoostClassifier | 0.53% | 0.50% |\n",
      "| GradientBoostingClassifier | 0.57% | 0.52% |\n",
      "| DecisionTreeClassifier | 0.57% | 0.51% |\n",
      "\n",
      "\n",
      "# the best classifier is `LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)` with train score of 0.5998858447488584 and test score of 0.5730593607305936\n",
      "\n",
      "---\n",
      "\n",
      "# training using `206` features with correlation >= 0.75\n",
      "| model | train score | test score|\n",
      "| --- | --- | --- |\n",
      "| LogisticRegression | 0.60% | 0.58% |\n",
      "| SGDClassifier | 0.59% | 0.58% |\n",
      "| SVC | 0.58% | 0.56% |\n",
      "| LinearSVC | 0.57% | 0.60% |\n",
      "| GaussianNB | 0.51% | 0.49% |\n",
      "| KNeighborsClassifier | 0.60% | 0.57% |\n",
      "| RandomForestClassifier | 0.58% | 0.55% |\n",
      "| AdaBoostClassifier | 0.53% | 0.48% |\n",
      "| GradientBoostingClassifier | 0.56% | 0.51% |\n",
      "| DecisionTreeClassifier | 0.56% | 0.50% |\n",
      "\n",
      "\n",
      "# the best classifier is `LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)` with train score of 0.6004566210045662 and test score of 0.5776255707762558\n",
      "\n",
      "---\n",
      "\n",
      "# training using `198` features with correlation >= 0.8\n",
      "| model | train score | test score|\n",
      "| --- | --- | --- |\n",
      "| LogisticRegression | 0.60% | 0.59% |\n",
      "| SGDClassifier | 0.57% | 0.60% |\n",
      "| SVC | 0.58% | 0.58% |\n",
      "| LinearSVC | 0.59% | 0.56% |\n",
      "| GaussianNB | 0.50% | 0.48% |\n",
      "| KNeighborsClassifier | 0.60% | 0.57% |\n",
      "| RandomForestClassifier | 0.57% | 0.54% |\n",
      "| AdaBoostClassifier | 0.55% | 0.47% |\n",
      "| GradientBoostingClassifier | 0.56% | 0.50% |\n",
      "| DecisionTreeClassifier | 0.54% | 0.50% |\n",
      "\n",
      "\n",
      "# the best classifier is `LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)` with train score of 0.5976027397260274 and test score of 0.589041095890411\n",
      "\n",
      "---\n",
      "\n",
      "# training using `180` features with correlation >= 0.9\n",
      "| model | train score | test score|\n",
      "| --- | --- | --- |\n",
      "| LogisticRegression | 0.60% | 0.58% |\n",
      "| SGDClassifier | 0.59% | 0.57% |\n",
      "| SVC | 0.59% | 0.58% |\n",
      "| LinearSVC | 0.57% | 0.59% |\n",
      "| GaussianNB | 0.50% | 0.49% |\n",
      "| KNeighborsClassifier | 0.60% | 0.55% |\n",
      "| RandomForestClassifier | 0.57% | 0.52% |\n",
      "| AdaBoostClassifier | 0.54% | 0.47% |\n",
      "| GradientBoostingClassifier | 0.55% | 0.51% |\n",
      "| DecisionTreeClassifier | 0.55% | 0.48% |\n",
      "\n",
      "\n",
      "# the best classifier is `LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)` with train score of 0.5998858447488584 and test score of 0.5799086757990868\n",
      "\n",
      "---\n",
      "\n",
      "CPU times: user 4min 39s, sys: 5.18 s, total: 4min 44s\n",
      "Wall time: 25min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for corr_threshold in [0.5, 0.7, 0.8, 0.9]:\n",
    "    # get feautres with high corr\n",
    "    corr_mat = df_poly_3.corr()['Avg']\n",
    "    feats = (corr_mat[abs(corr_mat)>=corr_threshold].index)\n",
    "    # now train \n",
    "    print(\"# training using `{}` features with correlation >= {}\".format(len(feats), corr_threshold))\n",
    "    auto_ml(df_poly_3[feats], df_poly_3['next_rate'])\n",
    "    print(\"\\n---\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training using `210` features with correlation >= 0.5\n",
    "| model | train score | test score|\n",
    "| --- | --- | --- |\n",
    "| LogisticRegression | 0.60% | 0.57% |\n",
    "| SGDClassifier | 0.59% | 0.59% |\n",
    "| SVC | 0.58% | 0.57% |\n",
    "| LinearSVC | 0.58% | 0.61% |\n",
    "| GaussianNB | 0.50% | 0.49% |\n",
    "| KNeighborsClassifier | 0.60% | 0.57% |\n",
    "| RandomForestClassifier | 0.58% | 0.56% |\n",
    "| AdaBoostClassifier | 0.53% | 0.48% |\n",
    "| GradientBoostingClassifier | 0.56% | 0.52% |\n",
    "| DecisionTreeClassifier | 0.57% | 0.52% |\n",
    "\n",
    "\n",
    "# the best classifier is `LogisticRegression` with train score of 0.60 and test score of 0.57\n",
    "\n",
    "model specs : LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True, intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1, penalty='l1', random_state=None, solver='liblinear', tol=0.0001, verbose=0, warm_start=False)\n",
    "\n",
    "---\n",
    "\n",
    "# training using `210` features with correlation >= 0.7\n",
    "| model | train score | test score|\n",
    "| --- | --- | --- |\n",
    "| LogisticRegression | 0.60% | 0.57% |\n",
    "| SGDClassifier | 0.59% | 0.58% |\n",
    "| SVC | 0.58% | 0.57% |\n",
    "| LinearSVC | 0.59% | 0.59% |\n",
    "| GaussianNB | 0.50% | 0.49% |\n",
    "| KNeighborsClassifier | 0.60% | 0.57% |\n",
    "| RandomForestClassifier | 0.58% | 0.54% |\n",
    "| AdaBoostClassifier | 0.53% | 0.50% |\n",
    "| GradientBoostingClassifier | 0.57% | 0.52% |\n",
    "| DecisionTreeClassifier | 0.57% | 0.51% |\n",
    "\n",
    "\n",
    "# the best classifier is `LogisticRegression` with train score of 0.599 and test score of 0.57\n",
    "\n",
    "model specs : LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
    "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
    "          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
    "          verbose=0, warm_start=False)\n",
    "\n",
    "---\n",
    "\n",
    "# training using `206` features with correlation >= 0.75\n",
    "| model | train score | test score|\n",
    "| --- | --- | --- |\n",
    "| LogisticRegression | 0.60% | 0.58% |\n",
    "| SGDClassifier | 0.59% | 0.58% |\n",
    "| SVC | 0.58% | 0.56% |\n",
    "| LinearSVC | 0.57% | 0.60% |\n",
    "| GaussianNB | 0.51% | 0.49% |\n",
    "| KNeighborsClassifier | 0.60% | 0.57% |\n",
    "| RandomForestClassifier | 0.58% | 0.55% |\n",
    "| AdaBoostClassifier | 0.53% | 0.48% |\n",
    "| GradientBoostingClassifier | 0.56% | 0.51% |\n",
    "| DecisionTreeClassifier | 0.56% | 0.50% |\n",
    "\n",
    "\n",
    "# the best classifier is `LogisticRegression` with train score of 0.60 and test score of 0.577\n",
    "\n",
    "model specs : LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
    "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
    "          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
    "          verbose=0, warm_start=False)\n",
    "\n",
    "---\n",
    "\n",
    "# training using `198` features with correlation >= 0.8\n",
    "| model | train score | test score|\n",
    "| --- | --- | --- |\n",
    "| LogisticRegression | 0.60% | 0.59% |\n",
    "| SGDClassifier | 0.57% | 0.60% |\n",
    "| SVC | 0.58% | 0.58% |\n",
    "| LinearSVC | 0.59% | 0.56% |\n",
    "| GaussianNB | 0.50% | 0.48% |\n",
    "| KNeighborsClassifier | 0.60% | 0.57% |\n",
    "| RandomForestClassifier | 0.57% | 0.54% |\n",
    "| AdaBoostClassifier | 0.55% | 0.47% |\n",
    "| GradientBoostingClassifier | 0.56% | 0.50% |\n",
    "| DecisionTreeClassifier | 0.54% | 0.50% |\n",
    "\n",
    "\n",
    "# the best classifier is `LogisticRegression` with train score of 0.597 and test score of 0.589\n",
    "\n",
    "model specs : LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
    "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
    "          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
    "          verbose=0, warm_start=False)\n",
    "\n",
    "---\n",
    "\n",
    "# training using `180` features with correlation >= 0.9\n",
    "| model | train score | test score|\n",
    "| --- | --- | --- |\n",
    "| LogisticRegression | 0.60% | 0.58% |\n",
    "| SGDClassifier | 0.59% | 0.57% |\n",
    "| SVC | 0.59% | 0.58% |\n",
    "| LinearSVC | 0.57% | 0.59% |\n",
    "| GaussianNB | 0.50% | 0.49% |\n",
    "| KNeighborsClassifier | 0.60% | 0.55% |\n",
    "| RandomForestClassifier | 0.57% | 0.52% |\n",
    "| AdaBoostClassifier | 0.54% | 0.47% |\n",
    "| GradientBoostingClassifier | 0.55% | 0.51% |\n",
    "| DecisionTreeClassifier | 0.55% | 0.48% |\n",
    "\n",
    "\n",
    "# the best classifier is `LogisticRegression` with train score of 0.599 and test score of 0.579\n",
    "\n",
    "model specs : LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
    "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
    "          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
    "          verbose=0, warm_start=False)\n",
    "\n",
    "---\n",
    "\n",
    "CPU times: user 4min 39s, sys: 5.18 s, total: 4min 44s\n",
    "Wall time: 25min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the classification report of the chosen model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 2190 entries, 2011-01-03 to 2016-12-31\n",
      "Freq: D\n",
      "Columns: 113 entries, Open to next_rate\n",
      "dtypes: float64(113)\n",
      "memory usage: 1.9 MB\n",
      "None\n",
      "model LogisticRegression has accuracy of 0.58(+/-)0.02%\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.57      0.67      0.62       214\n",
      "        1.0       0.62      0.53      0.57       224\n",
      "\n",
      "avg / total       0.60      0.60      0.59       438\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "model = LogisticRegression(C=100, penalty='l1')\n",
    "\n",
    "df = get_featured_frame('../res/EURUSD_15m_BID_01.01.2010-31.12.2016.csv')\n",
    "\n",
    "df = generate_poly_feats(df, degree=2)\n",
    "\n",
    "corr_mat = df.corr()['Avg']\n",
    "feats = (corr_mat[abs(corr_mat)>=0.8].index)\n",
    "    \n",
    "\n",
    "\n",
    "# split the data \n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(df[feats], df['next_rate'],\n",
    "                                                    test_size=.2, random_state=0)\n",
    "\n",
    "# let's get the cross validation score\n",
    "scores = cross_val_score(model, x_train, y_train, cv=5, n_jobs=-1)\n",
    "\n",
    "print(\"model LogisticRegression has accuracy of {:.2f}(+/-){:.2f}%\".format(scores.mean(), scores.std()))\n",
    "\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "y_pred = model.predict(x_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "from the previous experiments we can say that the best model is Logistic Regression, as it can actually find a good separation between the classes, and the best setup for the model is as follows :\n",
    "\n",
    "# Model : LogisticRegression\n",
    "\n",
    "## Parameters :\n",
    "\n",
    "`C` = 100\n",
    "`penalty` = `l1`\n",
    "\n",
    "# Features\n",
    "\n",
    "## Poly features \n",
    "\n",
    "we used the second degree polynomials to fit the non linear features\n",
    "\n",
    "## High correlation features\n",
    "\n",
    "we use the features with correlation above `0.8` with the dependent column\n",
    "\n",
    "# Scores\n",
    "\n",
    "## Best score using 5-fold cross validation\n",
    "\n",
    "the best score we have is `60%` accuracy\n",
    "\n",
    "## Precision and recall\n",
    "\n",
    "the model has a ***precision*** score of `0.60` and ***recall*** of `0.60`, and has an ***f1-score*** of `0.59`\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
